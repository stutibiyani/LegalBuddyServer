{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from pprint import pprint\n",
    "import spacy as spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from ipynb.fs.full.UnfairClauseCategorization import UnfairClauseCategorization \n",
    "from ipynb.fs.full.FairnessClassifierCNN import FairnessClassifierCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/FairnessClassifier.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    loaded_model_1 = model_from_json(data)\n",
    "loaded_model_1.load_weights(\"fairness_model_weights.h5\")\n",
    "\n",
    "with open(\"../models/UnfairClassifier.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    loaded_model_2 = model_from_json(data)\n",
    "loaded_model_2.load_weights(\"categorize_model_weights.h5\")\n",
    "\n",
    "fcc = FairnessClassifierCNN()\n",
    "ucc = UnfairClauseCategorization()\n",
    "\n",
    "class Predictions(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(num_words=5000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                          lower=True)\n",
    "        \n",
    "    def preprocess_text(self, df):\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "        brief_cleaning = (re.sub(\"[^A-Za-z]+\", ' ', str(row)).lower() for row in df['Clauses'])\n",
    "        txt = [self.cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "        return text\n",
    "    \n",
    "    def cleaning(self, doc):\n",
    "        txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        if len(txt) > 2:\n",
    "            return ' '.join(txt)\n",
    "        \n",
    "    def remove_punct(self, text):\n",
    "        text_nopunct = ''\n",
    "        text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "        text_nopunct = re.sub(r'\\s+', ' ', text_nopunct)\n",
    "        text_nopunct = re.sub(r'\\d+', '', text_nopunct)  #remove numbers\n",
    "        text_nopunct = text_nopunct.strip()              #remove whitespaces\n",
    "        return text_nopunct\n",
    "    \n",
    "    def lower_token(self, tokens): \n",
    "        return [w.lower() for w in tokens]   \n",
    "    \n",
    "    def removeStopWords(self, tokens): \n",
    "        stoplist = stopwords.words('english')\n",
    "        return [word for word in tokens if word not in stoplist]\n",
    "\n",
    "    def text_to_vec(self, df, maxlen):\n",
    "        clauses = df.Clauses.tolist()\n",
    "        self.tokenizer.fit_on_texts(clauses)\n",
    "        sequences = self.tokenizer.texts_to_sequences(clauses)\n",
    "        clauses = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "        return clauses\n",
    "\n",
    "    def get_prediction(self, loaded_model, df, maxlen):\n",
    "        df['text_clean'] = df['Clauses'].apply(lambda x: self.remove_punct(x))\n",
    "        tokens = [word_tokenize(sen) for sen in df.text_clean]\n",
    "        lower_tokens = [self.lower_token(token) for token in tokens]\n",
    "        filtered_words = [self.removeStopWords(sen) for sen in lower_tokens]\n",
    "        df['text_final'] = [' '.join(sen) for sen in filtered_words]\n",
    "        df['tokens'] = filtered_words\n",
    "        text = self.preprocess_text(df)\n",
    "        df.Clauses = df.text_final\n",
    "        df = df[['Clauses', 'tokens']]\n",
    "        \n",
    "        clauses = self.text_to_vec(df, maxlen)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(clauses)):\n",
    "            pred = loaded_model.predict(np.expand_dims(clauses[i], 0))\n",
    "            predictions.append(pred)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"page end by implied mail to implied mail content and infringement content\",\n",
    "        \"limit you and fullest may discretion this linden if any time content all of the other\",\n",
    "        \"by using or accessing the services , you agree to become bound by all the terms and conditions of this agreement.\",\n",
    "        \"9gag , inc reserves the right to remove any subscriber content from the site , suspend or \\\n",
    "         terminate subscriber 's right to use the services at any time , or pursue any other remedy or relief available \\\n",
    "         to 9gag , inc and/or the site under equity or law, for any reason -lrb- including , but not limited to , \\\n",
    "         upon receipt of claims or allegations from third parties or authorities relating to such subscriber content \\\n",
    "         or if 9gag , inc is concerned that subscriber may have breached the immediately preceding \\\n",
    "         sentence -rrb- , or for no reason at all . \",\n",
    "        \"this policy and consent forms part of our website terms of use and as such it shall \\\n",
    "         be governed by and construed in accordance with the laws of england and wales . \",\n",
    "        \"these pages , the content and infrastructure of these pages , and the online reservation service \\\n",
    "         provided on these pages and through the website are owned , operated and provided by booking.com b.v. \\\n",
    "         and are provided for your personal , non-commercial use only , subject to the terms and conditions set out below . \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(text)\n",
    "text_df.columns = ['Clauses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_fair = pred.get_prediction(loaded_model_1, text_df, 241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.01729887, 0.9812965 ]], dtype=float32),\n",
      " array([[0.9784586 , 0.01827978]], dtype=float32),\n",
      " array([[0.33864728, 0.70804775]], dtype=float32),\n",
      " array([[0.00655426, 0.9928636 ]], dtype=float32),\n",
      " array([[0.5858761, 0.4081908]], dtype=float32),\n",
      " array([[0.91976476, 0.09370353]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "pprint(predictions_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit fullest may discretion linden time content\n",
      "policy consent forms part website terms use shall governed construed accordance laws england wales\n",
      "pages content infrastructure pages online reservation service provided pages website owned operated provided bookingcom bv provided personal noncommercial use subject terms conditions set\n",
      "[array([[0.15396357, 0.26227072, 0.08719815, 0.04480341, 0.0781505 ,\n",
      "        0.09656006, 0.76138663, 0.22436924]], dtype=float32),\n",
      " array([[7.7725763e-06, 1.6078738e-03, 3.1382730e-04, 2.3260187e-04,\n",
      "        1.9302002e-04, 9.9998856e-01, 1.1387451e-03, 1.2150981e-05]],\n",
      "      dtype=float32),\n",
      " array([[0.00065664, 0.0006108 , 0.00088769, 0.07086726, 0.00070451,\n",
      "        0.01376947, 0.01656681, 0.05054652]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predictions_fair)):\n",
    "    if(predictions_fair[i][0][1].round() == 0):\n",
    "        print(text_df['Clauses'][i])\n",
    "        text_df = text_df.drop(i, axis=0)\n",
    "predictions_unfair = pred.get_prediction(loaded_model_2, text_df, 191)\n",
    "pprint(predictions_unfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
