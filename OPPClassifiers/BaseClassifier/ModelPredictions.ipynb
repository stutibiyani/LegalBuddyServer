{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f253a56bcacb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseClassifierCNN\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\BE_Project\\legalbuddyserver\\OPPClassifiers\\BaseClassifier\\BaseClassifierCNN.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m      \u001b[1;34m\"output_type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"stream\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m      \"text\": [\n\u001b[1;32m---> 12\u001b[1;33m       \u001b[1;34m\"Using TensorFlow backend.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m      ]\n\u001b[0;32m     14\u001b[0m     }\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from pprint import pprint\n",
    "import spacy as spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from ipynb.fs.full.BaseClassifierCNN import BaseClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../models/BaseClassifier.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    loaded_model = model_from_json(data)\n",
    "loaded_model.load_weights(\"base_classifier_weights.h5\")\n",
    "\n",
    "class Predictions(object):    \n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(num_words=5000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                          lower=True)\n",
    "        \n",
    "    def preprocess_text(self, df):\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "        brief_cleaning = (re.sub(\"[^A-Za-z]+\", ' ', str(row)).lower() for row in df['Clauses'])\n",
    "        txt = [self.cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "        return text\n",
    "    \n",
    "    def cleaning(self, doc):\n",
    "        txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        if len(txt) > 2:\n",
    "            return ' '.join(txt)\n",
    "        \n",
    "    def remove_punct(self, text):\n",
    "        text_nopunct = ''\n",
    "        text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "        text_nopunct = re.sub(r'\\s+', ' ', text_nopunct)\n",
    "        text_nopunct = re.sub(r'\\d+', '', text_nopunct)  #remove numbers\n",
    "        text_nopunct = text_nopunct.strip()              #remove whitespaces\n",
    "        return text_nopunct\n",
    "    \n",
    "    def lower_token(self, tokens): \n",
    "        return [w.lower() for w in tokens]   \n",
    "    \n",
    "    def removeStopWords(self, tokens): \n",
    "        stoplist = stopwords.words('english')\n",
    "        return [word for word in tokens if word not in stoplist]\n",
    "\n",
    "    def text_to_vec(self, df, maxlen):\n",
    "        clauses = df.Clauses.tolist()\n",
    "        self.tokenizer.fit_on_texts(clauses)\n",
    "        sequences = self.tokenizer.texts_to_sequences(clauses)\n",
    "        clauses = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "        return clauses\n",
    "\n",
    "    def get_prediction(self, loaded_model, df, maxlen):\n",
    "        df['text_clean'] = df['Clauses'].apply(lambda x: self.remove_punct(x))\n",
    "        tokens = [word_tokenize(sen) for sen in df.text_clean]\n",
    "        lower_tokens = [self.lower_token(token) for token in tokens]\n",
    "        filtered_words = [self.removeStopWords(sen) for sen in lower_tokens]\n",
    "        df['text_final'] = [' '.join(sen) for sen in filtered_words]\n",
    "        df['tokens'] = filtered_words\n",
    "        text = self.preprocess_text(df)\n",
    "        df.Clauses = df.text_final\n",
    "        df = df[['Clauses', 'tokens']]\n",
    "        \n",
    "        clauses = self.text_to_vec(df, maxlen)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(clauses)):\n",
    "            pred = loaded_model.predict(np.expand_dims(clauses[i], 0))\n",
    "            predictions.append(pred)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['use recommendation service email one articles friend need provide us friends email address email address return address automatically send person onetime email recommended article attached well invite friend visit register site use email addresses purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(text)\n",
    "text_df.columns = ['Clauses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ce36004c74d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Predictions' is not defined"
     ]
    }
   ],
   "source": [
    "pred = Predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred.get_prediction(loaded_model, text_df, 214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00350176, 0.00796415, 0.01493427, 0.54257953, 0.02125375,\n",
      "        0.8538634 , 0.01733487, 0.15754691, 0.00948425, 0.06927195]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bitf182560580114edf9527acde5c242c1f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
